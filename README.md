# Grid-Search

In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.

The traditional way of performing hyperparameter optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set[3] or evaluation on a held-out validation set.

Data used in this project are the data used in : 

1- Magumba, Mark Abraham, and Peter Nabende. "An ontology for generalized disease incidence detection on twitter." International Conference on Hybrid Artificial Intelligence Systems. Springer, Cham, 2017.


2-Magumba, Mark Abraham, Peter Nabende, and Ernest Mwebaze. "Message Classification for Generalized Disease Incidence Detection with Topologically Derived Concept Embeddings."
